{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "21a5903e-8d9d-401b-b1df-422255ae94a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Spark architecture (driver, executors, DAG)\n",
    "\n",
    "Databricks is essentially a managed, high-performance \"wrapper\" around Apache Spark. While Spark provides the distributed engine, Databricks adds a layer of automation, security (Unity Catalog), and a specialized engine called Photon to make it faster and easier to use.\n",
    "\n",
    "**1. The Driver (The Brain)**\n",
    "* The Driver is the central coordinator. When you run a cell in a Databricks notebook, you are talking directly to the Driver.\n",
    "* **Responsibilities:** It runs your main() function, creates the SparkSession, and maintains all the information about the Spark application.\n",
    "* **The Planner:** It translates your high-level code (Python, SQL, Scala) into a logical plan and then a physical execution plan.\n",
    "* **Task Master:** It breaks the work into small \"tasks\" and schedules them to be executed by the workers.\n",
    "* **State Keeper:** It tracks where data is located across the cluster and manages the overall lifecycle of the job.\n",
    "\n",
    "In modern Databricks, the Spark Connect architecture decouples the client from the Driver, allowing you to connect from any IDE or application with much better stability and \"versionless\" upgrades.\n",
    "\n",
    "**2. The Executors (The Muscles)**\n",
    "* Executors are the processes that live on the Worker Nodes. If the Driver is the architect, the Executors are the construction crew.\n",
    "* **Task Execution:** They receive tasks from the Driver, run the actual computation (filtering, joining, aggregating), and report the results back.\n",
    "* **Data Storage:** They store data in-memory or on disk for fast access. When you \"cache\" a DataFrame, it lives in the memory of these Executors.\n",
    "* **Isolation:** Each Spark application gets its own dedicated Executors. If one Executor crashes, the Driver simply restarts the task on a different one.\n",
    "\n",
    "**3. The DAG (The Blueprint)**\n",
    "* The Directed Acyclic Graph (DAG) is how Spark maps out the \"recipe\" for your data transformation.\n",
    "* **Directed:** The process flows in one direction (from input to output).\n",
    "* **Acyclic:** There are no loops. You can't go \"backward\" in the middle of an execution.\n",
    "* **Lazy Evaluation:** When you write df.filter(...), Spark doesn't actually do anything yet. It just adds a node to the DAG. It only starts working when you call an Action (like .show(), .count(), or .save()).\n",
    "\n",
    "**Stages & Tasks:**\n",
    "* **Stages:** The DAG is broken into stages based on \"Shuffle\" boundaries (whenever data needs to move between executors, like during a join or groupBy).\n",
    "* **Tasks:** Each stage is further broken into tasks—one task per data partition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2d293b80-221f-46a3-9228-1fb576d8be89",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### DataFrames vs RDDs\n",
    "\n",
    "**1. RDDs: The Foundational Layer**\n",
    "* An RDD is a distributed collection of elements. Because RDDs are \"opaque\" to Spark, the engine doesn't know what's inside your data—it just sees a collection of Java or Python objects.\n",
    "* Manual Control: You have to tell Spark exactly how to process data (e.g., map, flatMap, reduceByKey).\n",
    "* The Python Tax: In PySpark, RDDs are significantly slower because data must be serialized and moved between the Python process and the JVM.\n",
    "* When to use: Use RDDs only if you are working with unstructured data (like media files or raw text) or if you need very low-level control over partitioning that the DataFrame API doesn't provide.\n",
    "\n",
    "**2. DataFrames: The Optimized Evolution** \n",
    "* A DataFrame is essentially an RDD with a schema. Because Spark knows the data types and column names, it can use two secret weapons to make your code run faster:\n",
    "\n",
    "**The Catalyst Optimizer** \\\n",
    "When you write a DataFrame query, Spark doesn't just run it. It builds a Logical Plan, optimizes it (like pushing filters earlier to read less data), and creates an Optimized Physical Plan. \\\n",
    "**Example:**  If you filter a 1TB table for \"Country = 'USA'\", Catalyst ensures the filter happens at the source, so you don't actually pull 1TB into memory. \\\n",
    "**Project Tungsten** \n",
    "* This is Spark’s specialized memory management. Instead of using standard Java objects (which are heavy), Tungsten stores data in a compact binary format. This dramatically reduces the \"Garbage Collection\" overhead that often slows down big data jobs.\n",
    "\n",
    "**3. Which one should you use?**\n",
    "* Use DataFrames for: SQL queries, standard ETL, Machine Learning (MLlib), and virtually all structured data (Parquet, Delta, JSON, CSV).\n",
    "* Use RDDs for: Legacy code maintenance or complex \"black box\" algorithms where you need to manipulate raw Java/Python objects directly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "332a665e-34bb-47ef-86df-4a4174da5878",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Lazy evaluation\n",
    "\n",
    "In Apache Spark, Lazy Evaluation means that Spark does not execute your code immediately as you write it. Instead, it waits until the very last moment—when you actually need a result—to run the computation.\n",
    "\n",
    "**Transformations (The Planning)**\n",
    "* Transformations are instructions that tell Spark how to change the data. When you call a transformation, Spark simply records the instruction in the DAG (Directed Acyclic Graph).\n",
    "* **Examples:** .filter(), .select(), .join(), .groupBy(), .map().\n",
    "* **Result:** These return a new DataFrame but do zero actual work on the data.\n",
    "\n",
    "**Actions (The Execution)**\n",
    "* Actions are the \"triggers.\" When an action is called, Spark looks at the list of transformations it has collected, optimizes them, and sends the tasks to the executors to get the work done.\n",
    "* **Examples:** .show(), .count(), .collect(), .save(), .write().\n",
    "* **Result:** These trigger the actual computation and return a value to the driver or write data to storage.\n",
    "\n",
    "**Benefits**\n",
    "* **A. Query Optimization**: Because Spark sees the \"whole picture\" (the entire DAG) before it starts, the Catalyst Optimizer can rearrange your steps. \n",
    "* **Predicate Pushdown:** If you have a 100GB table and you apply a filter at the very end of your script, Spark is smart enough to apply that filter while reading the data. This means it only pulls the data it needs into memory, rather than loading the full 100GB and filtering it later.\n",
    "\n",
    "* **B. Fault Tolerance**\n",
    "* If a worker node fails in the middle of a job, Spark doesn't panic. Because it has the \"lineage\" (the DAG recipe), it knows exactly how to re-create the lost data on a new worker from the original source.\n",
    "\n",
    "* **C. Reducing Pass-Throughs**\n",
    "* Instead of writing temporary results to disk after every single line of code, Spark combines multiple transformations into a single \"stage,\" keeping data in memory as much as possible.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "23445c45-8029-474a-abd0-c6aaa7f479fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Notebook magic commands (%sql, %python, %fs)\n",
    "\n",
    "In Databricks, Magic Commands allow you to switch languages, interact with the file system, or run shell scripts—all within the same notebook.\n",
    "\n",
    "#### 1. Language Magics\n",
    "* %python: Executes Python code.\n",
    "* %sql: Executes Spark SQL queries.\n",
    "* %scala: Executes Scala code.\n",
    "* %r: Executes R code.\n",
    "* %sh: Executes shell commands.\n",
    "* %fs: Executes file system commands.\n",
    "* %md: Renders Markdown text.\n",
    "\n",
    "#### 2. File System Magics: It allows you to interact with the Databricks File System (DBFS) without writing complex code.\n",
    "\n",
    "- %fs ls - Lists files in a directory - %fs ls /databricks-datasets\n",
    "- %fs cp - Copies a file or directory. - %fs cp /src/file.txt /dest/file.txt\n",
    "- %fs mv - Moves or renames a file. - %fs mv /old/path /new/path\n",
    "- %fs rm - Removes a file or directory. - %fs rm -r /path/to/directory\n",
    "- %fs mkdirs - Creates a new directory. - %fs mkdirs /mnt/new_folder\n",
    "- %fs head - Displays the first few bytes of a file. - %fs head /mnt/data/logs.txt\n",
    "\n",
    "#### 3. Shell & OS Magics (%sh): Databricks nodes run on Linux. If you need to perform OS-level tasks, use %sh.\n",
    "\n",
    "- %sh: Runs standard Bash commands.\n",
    "- Example: %sh pip list or %sh wget https://example.com/data.csv\n",
    "- %sh top: Can be used to check memory/CPU usage on the driver node.\n",
    "\n",
    "#### 4. Workflow & Utility Magics: These commands help you manage your notebook environment and dependencies.\n",
    "\n",
    "- %run <path>: Executes another notebook and imports its variables, functions, and widgets into your current session.\n",
    "- Common use: Running a Config or Functions notebook at the start of a project.\n",
    "- %pip: Used to install Python libraries specific to the current notebook session.\n",
    "- Example: %pip install seaborn\n",
    "- %md: Renders Markdown for documentation (titles, lists, images, and LaTeX).\n",
    "- %load_ext: Used to load IPython extensions (like the SQL profiler).\n",
    "\n",
    "#### 5. Data Visualization & Widgets\n",
    "- dbutils.widgets: Not a magic command, but allows you to create interactive dropdowns and text boxes at the top of your notebook.\n",
    "- display(): A function (not a magic) that renders DataFrames as rich, interactive tables and charts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "05147745-c3c8-42ea-8542-de20cb73dbcd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Tasks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21fdff05-8ac7-4cf3-8ccd-df1b94677718",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define the path to your downloaded CSV\n",
    "file_path = \"/Volumes/workspace/ecommerce/ecommerce_data/2019-Oct.csv\"\n",
    "\n",
    "# Read the file with correct options\n",
    "df = (spark.read\n",
    "      .format(\"csv\")\n",
    "      .option(\"header\", \"true\")        # Uses the first row as column names\n",
    "      .option(\"inferSchema\", \"true\")   # Automatically detects data types (e.g., price as double)\n",
    "      .load(file_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8bbc1276-0e7b-41f0-ab1e-5d692d8ed745",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Verify the result\n",
    "df.printSchema()\n",
    "display(df.limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5a8f6f4-b642-4c1f-a15e-5097636a63c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## first 10 rows with selected columns\n",
    "\n",
    "df.select(\"event_time\",\"product_id\",\"brand\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da888165-729c-4df6-84e3-b1e73057eceb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## filter the data\n",
    "\n",
    "df.filter(\"price>100\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c656a2a6-f277-4aed-8e62-f930c0c4efdc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## groupby\n",
    "\n",
    "df.groupBy(\"event_type\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dca96097-f72c-4b81-a144-97e2cf686b01",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## group by and order by\n",
    "\n",
    "top_brands = df.groupBy(\"brand\").count().orderBy(\"count\", ascending=False).limit(5)\n",
    "top_brands.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2a9545c-ac7f-40de-806f-859eda512d13",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "top_brands.write.mode(\"append\").saveAsTable(\"workspace.ecommerce.top_brands_output\")\n",
    "print(\"Appended top brands to managed table: workspace.ecommerce.top_brands_output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "68bb4311-1107-495e-a139-25af0a3b81cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Day_2_Apache_Spark_Fundamentals",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
