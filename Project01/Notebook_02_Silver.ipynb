{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ffd09a90-b07d-4ab9-b001-7e663247d1c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. Setup Widgets for Batch Processing\n",
    "dbutils.widgets.text(\"run_date\", \"2026-01-15\")\n",
    "v_date = dbutils.widgets.get(\"run_date\")\n",
    "\n",
    "from pyspark.sql.functions import col, to_timestamp, split, to_date, lit\n",
    "\n",
    "# 2. Read from Bronze (Filtering by batch date)\n",
    "# This ensures we only process the new data for the day\n",
    "bronze_df = spark.table(\"ecommerce_prod.bronze.raw_events\").filter(col(\"batch_date\") == v_date)\n",
    "\n",
    "# 3. Data Cleaning & Transformation\n",
    "silver_df = (bronze_df\n",
    "    # Convert event_time string to a real Timestamp\n",
    "    .withColumn(\"event_time\", to_timestamp(col(\"event_time\")))\n",
    "    # Create a Date column for efficient partitioning\n",
    "    .withColumn(\"event_date\", to_date(col(\"event_time\")))\n",
    "    # Split category_code into main and sub categories\n",
    "    .withColumn(\"main_category\", split(col(\"category_code\"), r\"\\.\").getItem(0))\n",
    "    .withColumn(\"sub_category\", split(col(\"category_code\"), r\"\\.\").getItem(1))\n",
    "    # Remove exact duplicates (common in clickstream data)\n",
    "    .dropDuplicates([\"event_time\", \"user_id\", \"product_id\"])\n",
    "    # Add a processing timestamp for auditing\n",
    "    .withColumn(\"processed_at\", lit(v_date))\n",
    ")\n",
    "\n",
    "# 4. Write to Silver Table with Performance Options\n",
    "# 'overwriteSchema' allows us to add the main_category columns if they don't exist\n",
    "# 'partitionBy' ensures that future queries only read the specific dates needed\n",
    "silver_df.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .partitionBy(\"event_date\") \\\n",
    "    .saveAsTable(\"ecommerce_prod.silver.cleaned_events\")\n",
    "\n",
    "# 5. Optimize & Z-Order (The Task 3 Performance Boost)\n",
    "# We run this to physically reorganize the 42M rows by 'brand'\n",
    "spark.sql(f\"OPTIMIZE ecommerce_prod.silver.cleaned_events ZORDER BY (brand)\")\n",
    "\n",
    "print(f\"âœ… Silver layer complete and Z-Ordered for {v_date}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2949db75-8384-4854-84ea-0cd5cffce4c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Notebook_02_Silver",
   "widgets": {
    "run_date": {
     "currentValue": "2026-01-15",
     "nuid": "b03c8fd4-8eac-4499-9277-79916f2347cb",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "2026-01-15",
      "label": null,
      "name": "run_date",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "2026-01-15",
      "label": null,
      "name": "run_date",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
