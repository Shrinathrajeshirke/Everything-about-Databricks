{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "48693769-36a2-43a6-956b-9ec62bb880af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Why Databricks vs Pandas/Hadoop?\n",
    "\n",
    "1. Databricks vs. Pandas\n",
    "The main difference is Scalability. \n",
    "Pandas: Processes data in-memory on a single machine (your laptop or the \"driver\" node in a cluster). If your dataset exceeds your RAM, Pandas will crash. It is ideal for small datasets (typically < 1–2 GB) and quick exploration.\n",
    "Databricks (Spark): Processes data in a distributed manner across many machines (nodes). It can handle terabytes or petabytes by splitting data into partitions and processing them in parallel.\n",
    "Key Mindset Shift: Spark uses lazy evaluation, meaning it builds an execution plan and only runs it when you call an action (like .show()), allowing it to optimize performance behind the scenes. \n",
    "\n",
    "2. Databricks vs. Hadoop\n",
    "The main difference is Speed and Modernity. \n",
    "Hadoop: A legacy ecosystem that stores data on disk (HDFS) and uses MapReduce for batch processing. It is often slower because it reads/writes to disk frequently.\n",
    "Databricks: Built on Apache Spark, which performs computations in-memory, making it up to 100x faster than Hadoop for many tasks.\n",
    "Architecture: Databricks uses a \"Lakehouse\" architecture, combining the cheap storage of a data lake with the high-performance features (like ACID transactions) of a data warehouse. Hadoop is typically a \"data swamp\" without these managed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6f5f779d-24fa-47c0-a522-595f1fa15da6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Lakehouse architecture basics\n",
    "\n",
    "The Lakehouse architecture is a modern data management approach that combines the best features of two legacy systems: **data lakes and data warehouses**. It provides a single platform that offers the scale and flexibility of a data lake with the structure and ACID compliance (Atomicity, Consistency, Isolation, Durability) of a data warehouse.\n",
    "Here are the four fundamental basics of the Lakehouse architecture:\n",
    "\n",
    "**1. Single Source of Truth (The Data Lake Foundation)**\n",
    "* The foundation of the Lakehouse is a vast, cheap, and open data lake (usually in cloud storage like AWS S3, Azure Data Lake Storage, or GCS).\n",
    "* Open Formats: Data is stored in open, standardized formats like Parquet or ORC.\n",
    "* Raw Data Storage: It stores all data—structured, semi-structured (JSON), and unstructured (images, audio)—regardless of its initial use case, creating a single, centralized data repository.\n",
    "\n",
    "**2. Introduction of the \"Delta\" Layer (ACID Transactions)**\n",
    "* This is the key innovation that differentiates a Lakehouse from a raw data lake. A transactional storage layer (most popularly implemented using Delta Lake on Databricks) is added on top of the open data format files.\n",
    "* This layer provides:\n",
    "* **ACID Compliance:** Ensures reliable data writes and reads, preventing data corruption that can happen in raw data lakes during concurrent operations.\n",
    "* **Schema Enforcement:** Guarantees data quality by automatically preventing \"bad\" records from entering a table, which is crucial for reliable analytics.\n",
    "* **Time Travel:** Allows you to query historical versions of your data, making audits easier and simplifying the rollback of bad data changes.\n",
    "\n",
    "**3. Support for Diverse Workloads (Unified Platform)**\n",
    "* A Lakehouse breaks down data silos by supporting all data functions in one place. You no longer need separate systems for different teams:\n",
    "* **Data Engineering (ETL):** Processing and cleaning raw data reliably.\n",
    "* **Data Warehousing (BI/SQL):** Running traditional business intelligence (BI) and reporting queries directly on the lakehouse tables.\n",
    "* **Data Science & Machine Learning:** Accessing rich, feature-engineered data using Python/R/Scala libraries directly from the same platform.\n",
    "\n",
    "**4. Decoupled Storage and Compute**\n",
    "* In a Lakehouse, your storage (the data files in the cloud) is separate from your computing power (the Databricks cluster that runs queries).\n",
    "* You can scale your storage capacity infinitely and cheaply without affecting your processing speed.\n",
    "* You can spin up large clusters for heavy lifting and shut them down when idle, saving significant costs while always having access to all your data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bcc59ad5-4986-475c-a19e-4fcc74f9103a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Databricks workspace structure\n",
    "\n",
    "The Databricks Workspace is organized around **Unity Catalog**, which has shifted the focus from local folders to a **governed, three-tier architecture.**\n",
    "\n",
    "Here is the breakdown of how your workspace is structured:\n",
    "\n",
    "**1. The Three-Tier Data Hierarchy (Unity Catalog)** \\\n",
    "This is how you organize your actual data tables and files: \n",
    "* **Catalog:** The top level (e.g., prod_catalog). It represents a business unit or environment.\n",
    "* **Schema (Database):** The middle level (e.g., ecommerce_data). It groups related tables together.\n",
    "* **Volume / Table:** The bottom level. **Tables:** Structured data you query with SQL. **Volumes:** Folders for non-tabular files (like your Kaggle CSVs or images).\n",
    "\n",
    "**2. Workspace Assets (The Sidebar)** \\\n",
    "On the left-hand menu, you’ll find the functional areas: \n",
    "* **Workspace (Folders):** This is where your Notebooks, Libraries, and Git Folders live. \\\n",
    "**Shared:** For team collaboration.\n",
    "**Users:** Your private sandbox.\n",
    "* **Catalog Explorer:** The UI to manage permissions, schemas, and data lineage.\n",
    "* **Compute:** Where you create and manage your clusters (SQL Warehouses for BI, or All-Purpose clusters for Data Science).\n",
    "* **Workflows:** The scheduling engine where you turn notebooks into automated \"Jobs.\"\n",
    "\n",
    "**3. Medallion Architecture (The Logical Flow)** \\\n",
    "While not a physical \"folder,\" most Databricks workspaces are logically structured using the Medallion pattern to move data through stages of quality: \\\n",
    "**Bronze (Raw):** Your initial Kaggle download (the raw CSVs in a Volume). \\\n",
    "**Silver (Cleaned):** Data after you fix the schema, handle nulls, and format timestamps. \\\n",
    "**Gold (Aggregated):** Business-ready tables (e.g., \"Daily Sales Summary\") used for PowerBI or dashboards.\n",
    "\n",
    "**4. Git Folders (Repos)** \\\n",
    "these are special folders in your workspace synced with GitHub. They allow you to treat your notebooks as code, providing version control that standard workspace folders lack."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fb13c3e2-6ae8-45e2-a973-10fa2a19f307",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Industry use cases (Netflix, Shell, Comcast)\n",
    "\n",
    "**1. Netflix: Personalized Real-Time Recommendations**\n",
    "\n",
    "* Netflix uses Databricks to manage its massive telemetry data (every click, pause, and play).\n",
    "* **The Use Case:** They moved from a messy data lake to a Lakehouse to enable Real-time Personalization.\n",
    "* **How it Works:** By using Spark and Delta Lake, they process billions of events per second to update your \"Top Picks\" instantly.\n",
    "* **The Result:** Reduced \"time-to-insight\" from hours to seconds, ensuring you stay on the app by seeing the most relevant content immediately. \n",
    "\n",
    "**2. Shell: Predictive Maintenance & Energy Transition**\n",
    "\n",
    "Shell operates over 40,000 gas stations and thousands of industrial assets. \\\n",
    "* **The Use Case:** Predictive Maintenance on a global scale. They monitor over 2 million sensors across their infrastructure.\n",
    "* **How it Works:** Databricks processes sensor data to predict when a valve or pump might fail before it happens. This prevents oil spills and expensive downtime.\n",
    "* **The Result:** Saved millions in operational costs and accelerated their move toward renewable energy by optimizing wind farm performance using the same data models. \n",
    "\n",
    "**3. Comcast: Real-Time Voice Search & Customer Experience**\n",
    "\n",
    "If you speak into an Xfinity remote, you are likely interacting with a Databricks-powered pipeline. \\\n",
    "* **The Use Case:** Natural Language Processing (NLP) and unified customer views.\n",
    "* **How it Works:** Comcast uses Databricks to unify data from millions of set-top boxes. They run machine learning models that interpret voice commands in real-time and predict when a customer might need technical support before they even call.\n",
    "* **The Result:** A significantly higher success rate for voice commands and a proactive customer service model that reduces \"churn\" (customers leaving the service). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c048ad73-9649-466e-a804-6d3cfa99aaae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Basic Pyspark commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5e1e1fc-aa75-43e4-bd81-42baff9fceb5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define the path to your downloaded CSV\n",
    "file_path = \"/Volumes/workspace/ecommerce/ecommerce_data/2019-Oct.csv\"\n",
    "\n",
    "# Read the file with correct options\n",
    "df = (spark.read\n",
    "      .format(\"csv\")\n",
    "      .option(\"header\", \"true\")        # Uses the first row as column names\n",
    "      .option(\"inferSchema\", \"true\")   # Automatically detects data types (e.g., price as double)\n",
    "      .load(file_path))\n",
    "\n",
    "# Verify the result\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9fea982c-24c3-48fd-a74a-9cfa6c0a98de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Day_1_Basics_of_Databrics",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
