{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d88b7642-fa41-48a1-a766-95b32d222932",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Databricks Jobs vs notebooks\n",
    "\n",
    "**Notebooks: The \"Drafting Board\"**\n",
    "- Notebooks are interactive. You use them to experiment, debug, and visualize data in real-time.\n",
    "- **Best for:** Development, ad-hoc analysis, and \"exploring\" the data.\n",
    "- **How they run:** They usually run on All-Purpose Clusters, which are always \"on\" and more expensive because they are optimized for quick user feedback.\n",
    "- **The \"Human\" Element:** You manually click \"Run Cell\" to see what happens.\n",
    "\n",
    "- **Databricks Jobs: The \"Production Line\"**\n",
    "- A Job is an automated way to run your code on a schedule or in response to an event (like a new file arriving).\n",
    "- **Best for:** Production pipelines, daily reporting, and the Medallion architecture.\n",
    "- **How they run:** They typically run on Job Clusters. These clusters start up only when the job begins and shut down immediately after it finishes.\n",
    "- **The \"System\" Element:** They run automatically at 2:00 AM while you sleep.\n",
    "\n",
    "**Key Comparison**\n",
    "| Feature | Notebook (Interactive) | Databricks Job (Automated)\n",
    "| ----- | ----- | ----- |\n",
    "| Primary Use | Development / Debugging | Production Pipelines\n",
    "| Cluster Type | All-Purpose (Expensive) | Job Cluster (Cheaper/Efficient)\n",
    "| Execution | Manual (Cell by Cell) | Scheduled or Triggered \n",
    "| Reliability | Good for testing | High (Built-in retries & alerts) \n",
    "| Cost | High (Clusters stay active) | Low (Clusters only exist during run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "efaa826d-7f24-486e-b279-c46b04162ddd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Multi-task workflows\n",
    "\n",
    "- In Databricks, a Multi-task Workflow is the orchestration layer that connects your individual notebooks into a single, automated pipeline.\n",
    "- Instead of manually running your Bronze notebook, then your Silver notebook, and finally your Gold notebook, you create a Directed Acyclic Graph (DAG). \n",
    "- This ensures that Task B only starts if Task A finishes successfully.\n",
    "- **How a Multi-task Workflow looks:** \n",
    "- For the Medallion architecture, your workflow would look like a chain of dependencies:\n",
    "- Task 1 (Bronze): Ingest raw files from S3/ADLS.\n",
    "- Task 2 (Silver): Clean data and split categories (Depends on Task 1).\n",
    "- Task 3 (Gold): Aggregate values (Depends on Task 2).\n",
    "\n",
    "**Key Features of Multi-task Workflows**\n",
    "- **Task Dependencies:** You can set \"upstream\" tasks. If the Silver cleaning fails due to a data quality issue, the Gold aggregation will never start, preventing \"bad data\" from reaching the business.\n",
    "- **Parallel Execution:** If you have two different Gold tables, you can run them both at the same time after the Silver task finishes to save time.\n",
    "- **Parameters:** You can pass values between tasks. For example, you can pass a \"processing_date\" from the first task to all subsequent ones.\n",
    "- **Repair Run:** This is a lifesaver. If your workflow has 10 tasks and fails at Task 9, you can \"Repair\" it, and Databricks will only re-run the failed tasks, saving you money and time.\n",
    "\n",
    "**Why use Multi-task Jobs over one giant Notebook?**\n",
    "| Feature | Single Giant Notebook | Multi-task Workflow | \n",
    "| ----- | ----- | ----- |\n",
    "| Debugging | Hard to find where it failed in 1000 lines. | You see exactly which \"Task\" failed in the UI.\n",
    "| Compute | Uses one cluster for everything. | Can use different cluster sizes for different tasks.\n",
    "| Retry Logic | Fails entirely. | You can set specific retries for \"flaky\" source systems. \n",
    "| Collaboration | Hard for multiple people to edit. | Different team members can own different notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "114a3d66-8e8b-4919-8a59-cba5ec1b6b26",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Parameters & scheduling\n",
    "\n",
    "- Parameters make your code flexible, and Scheduling makes it autonomous.\n",
    "\n",
    "**1. Parameters (Widgets)**\n",
    "- Instead of hard-coding values like a specific date or a file path, you use Widgets. \n",
    "- This allows you to run the same notebook for different scenarios without changing the code.\n",
    "- In Databricks, you define a parameter at the top of your notebook:\n",
    "```\n",
    "# Create a text widget for the processing date\n",
    "dbutils.widgets.text(\"processing_date\", \"2026-01-14\")\n",
    "\n",
    "# Pull the value into a variable\n",
    "run_date = dbutils.widgets.get(\"processing_date\")\n",
    "\n",
    "# Use it in your Silver filter\n",
    "silver_df = bronze_df.filter(col(\"event_time\").cast(\"date\") == run_date)\n",
    "```\n",
    "\n",
    "**Why use this?**\n",
    "- **Backfilling:*** If you need to re-run data for last month, you just change the parameter value in the Job UI rather than editing your Python code.\n",
    "- **Dynamic Paths:** You can pass the input folder path as a parameter.\n",
    "\n",
    "**2. Scheduling (Triggering)**\n",
    "- Once your Multi-task Workflow is built, you need to tell Databricks when to run it. You have three main options:\n",
    "\n",
    "**A. Scheduled (Cron)**: The most common. You set a specific time (e.g., \"Every day at 6:00 AM\").\n",
    "- Use Case: Daily business reports for the Gold layer.\n",
    "\n",
    "**B. File Arrival (Trigger)**: Databricks monitors a folder in your cloud storage (S3/ADLS). As soon as a new file lands, the Job starts.\n",
    "- Use Case: Real-time or near-real-time ingestion.\n",
    "\n",
    "**C. Continuous**: The job starts again as soon as the previous run finishes.\n",
    "Use Case: High-frequency streaming data.\n",
    "\n",
    "**3. Putting it together in a Workflow**\n",
    "- When you configure the Job Task, you map these together:\n",
    "- Select Notebook: Silver_Layer_Notebook\n",
    "- Parameters: Key: processing_date, Value: {{now() | date('yyyy-MM-dd')}} (This is a dynamic value that always passes \"today\").\n",
    "- Schedule: Set to Daily, 02:00 AM.\n",
    "\n",
    "**The \"Production\" Benefit**\n",
    "- By combining Multi-tasking, Parameters, and Scheduling:\n",
    "- Error Handling: You can set \"Retries\" (e.g., \"If Task 1 fails, try again 3 times every 5 minutes\").\n",
    "- Notifications: You can set an alert to email you only if the Job fails.\n",
    "- Cost Tracking: You can tag the Job so you know exactly how much money the \"Medallion Pipeline\" is costing each month."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9fb1c1d2-c449-4489-9391-4b3da784bf83",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Error handling\n",
    "\n",
    "**1. Job-Level Error Handling (The Safety Net)**\n",
    "- Databricks Workflows provide built-in settings so you don't have to write complex \"retry\" logic yourself.\n",
    "- Retries: You can configure a task to \"Retry on failure.\" For example, if a cloud connection drops, Spark can wait 5 minutes and try again.\n",
    "- Timeouts: Prevent a \"stuck\" job from running forever and costing money by setting a maximum duration (e.g., 2 hours).\n",
    "- On-Failure Tasks: You can create a \"Conditional Task.\" If the Silver layer fails, it triggers a \"Cleanup\" task or sends a high-priority alert to Slack/PagerDuty.\n",
    "\n",
    "**2. Code-Level Error Handling (The Surgeon)**\n",
    "- In your Notebooks, you use Python's try-except blocks to handle specific data issues without crashing the whole pipeline.\n",
    "- Example: Handling a Missing File\n",
    "```\n",
    "try:\n",
    "    df = spark.read.format(\"parquet\").load(path)\n",
    "except Exception as e:\n",
    "    if \"Path does not exist\" in str(e):\n",
    "        print(\"No new data found today. Skipping Task.\")\n",
    "        dbutils.jobs.taskValues.set(key=\"status\", value=\"no_data\")\n",
    "        # Gracefully exit without failing the whole job\n",
    "        dbutils.notebook.exit(\"Success: No data to process\")\n",
    "    else:\n",
    "        raise e # Re-throw if it's a real error\n",
    "```\n",
    "\n",
    "**3. Expectations & Data Quality (The Filter)**\n",
    "- For the Medallion architecture, you want to catch \"bad data\" before it ruins your Gold aggregates.\n",
    "- Filter Logic: Instead of letting the code crash on a null price, you can redirect bad rows to a Quarantine Table.\n",
    "- Delta Expectations: If you are using Delta Live Tables (DLT), you can set rules like CONSTRAINT valid_price EXPECT (price > 0) ON VIOLATION DROP ROW.\n",
    "\n",
    "**4. Notifications (The Alarm)**\n",
    "- Don't wait to check the dashboard. In the Workflows UI, you can set Notifications:\n",
    "- On Start: Useful for long-running batch jobs.\n",
    "- On Success: Good for peace of mind.\n",
    "- On Failure: Critical. This sends the error log directly to your email or a webhook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7e53a7ed-ca31-48a2-a3b1-bd12e810eca2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Task 1: Add parameter widgets to notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "58aa15ab-4996-4dfc-8bf0-476022e791e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Day_7_Workflows_Jobs",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
