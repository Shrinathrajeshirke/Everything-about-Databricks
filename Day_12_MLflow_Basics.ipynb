{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "46fa5858-2a15-49bf-b538-d6f0ce6b2c80",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### MLflow components (tracking, registry, models)\n",
    "\n",
    "**1. MLflow Tracking (The Experiment Journal)**\n",
    "- When you train a model, you usually try different settings (hyperparameters). If you don't track them, you'll forget which settings gave you the best results.\n",
    "- What it tracks:\n",
    "- **Parameters:** Like the \"learning rate\" or \"number of trees\" in a forest.\n",
    "- **Metrics:** Your Accuracy, RMSE, or P-value.\n",
    "- **Artifacts:** The actual model file, plots, or images.\n",
    "- **The Benefit:** You can compare 50 different runs side-by-side in a table to see which one is the winner.\n",
    "\n",
    "**2. MLflow Models (The Universal Package)**\n",
    "- Once you have a trained model, it needs to be saved. A \"Model\" in MLflow isn't just a file; it’s a standardized folder.\n",
    "- What it does: It packages your model so it can be run anywhere (in a Notebook, as a Real-time API, or as a Batch job).\n",
    "- The Benefit: You don't have to worry about whether the model was made in Scikit-Learn, PyTorch, or SparkML. MLflow handles the \"flavor\" so it works across different systems.\n",
    "\n",
    "**3. MLflow Model Registry (The Version Controller)**\n",
    "- This is like GitHub for Models. It manages the lifecycle of your model from \"Birth\" to \"Retirement.\"\n",
    "- **Stages:**\n",
    "- None: Just saved.\n",
    "- Staging: Testing the model to see if it works on real data.\n",
    "- Production: The model is currently powering your dashboard or website.\n",
    "- Archived: The model is old and has been replaced.\n",
    "- The Benefit: If a new model starts making mistakes, you can \"roll back\" to the previous version with one click.\n",
    "\n",
    "**4. How this looks in your Project**\n",
    "- Imagine you are predicting if a user will buy a Samsung phone:\n",
    "- Tracking: You run 10 experiments. Run #7 has 92% accuracy. You tag it as \"Best Run.\"\n",
    "- Models: You save Run #7 as a standardized MLflow model.\n",
    "- Registry: You move that model to \"Production.\" Now, your Gold layer can use that model to label users as \"Likely Buyers\" every morning.\n",
    "\n",
    "**5. Why use it with a SQL Warehouse?**\n",
    "Databricks integrates MLflow directly. You can actually call your registered models directly inside a SQL query!\n",
    "```\n",
    "SQL\n",
    "-- Example: Using a registered model in SQL\n",
    "SELECT \n",
    "  user_id,\n",
    "  predict_purchase(feature1, feature2) as likelihood\n",
    "FROM ecommerce_prod.gold.user_ml_features;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d50573fd-4eb2-46e2-8113-85d2be856a71",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Experiment tracking\n",
    "\n",
    "**1. What exactly is being \"Tracked\"?**\n",
    "- When you start an \"Experiment,\" MLflow captures four main things:\n",
    "- **Parameters:** Your input settings (e.g., \"I used 100 trees for my Random Forest\").\n",
    "- **Metrics:** Your results (e.g., \"The accuracy was 85%\").\n",
    "- **Artifacts:** Your files (e.g., a \"Importance Plot\" chart or the actual model file).\n",
    "- **Source:** Which Notebook and which version of the code produced this result.\n",
    "\n",
    "**2. How to run an Experiment (The Code)**\n",
    "- Here is how you would track a simple model that predicts if a user will make a purchase.\n",
    "```\n",
    "Python\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 1. Start the experiment\n",
    "with mlflow.start_run(run_name=\"Purchase_Predictor_v1\"):\n",
    "    \n",
    "    # 2. Log a parameter (your \"recipe\")\n",
    "    n_estimators = 100\n",
    "    mlflow.log_param(\"trees\", n_estimators)\n",
    "    \n",
    "    # 3. Train your model\n",
    "    model = RandomForestClassifier(n_estimators=n_estimators)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # 4. Log a metric (your \"score\")\n",
    "    acc = accuracy_score(y_test, model.predict(X_test))\n",
    "    mlflow.log_metric(\"accuracy\", acc)\n",
    "    \n",
    "    # 5. Save the model (the \"artifact\")\n",
    "    mlflow.sklearn.log_model(model, \"model\")\n",
    "\n",
    "print(f\"Run complete! Accuracy: {acc}\")\n",
    "```\n",
    "\n",
    "**3. Why this is a game-changer for you**\n",
    "- Imagine you spend all day Friday testing different models. On Monday, your boss asks for the best one.\n",
    "- Without Tracking: You have to look through messy notebooks or try to remember.\n",
    "- With Tracking: You open the Experiments sidebar in Databricks, sort by \"Accuracy,\" and the winner is right at the top.\n",
    "\n",
    "**4. Comparing Runs**\n",
    "- The most powerful feature of tracking is the Comparison View. You can select 3 or 4 different runs and see a \"Parallel Coordinates Plot.\" \n",
    "- This shows you exactly how changing one parameter (like the number of trees) caused the accuracy to go up or down."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "084e7b43-fe9b-4267-a2b4-fd7dbbfbf6a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Model logging\n",
    "\n",
    "**1. What's inside a \"Logged Model\"?**\n",
    "- When you use mlflow.sklearn.log_model(), MLflow creates a folder containing:\n",
    "- **The Model File:** The actual serialized code (like a .pkl or .joblib file).\n",
    "- **MLmodel file:** A text file that tells MLflow how to load the model (e.g., \"This is a Scikit-Learn model\").\n",
    "- **Conda/Requirements:** A list of every library and version (like pandas==2.0.0) needed to make the model work again.\n",
    "\n",
    "**2. How to Log your Model**\n",
    "- In your project, after you train your classifier on the 42M row dataset, you log it like this:\n",
    "```\n",
    "Python\n",
    "import mlflow.sklearn\n",
    "\n",
    "# Start a run\n",
    "with mlflow.start_run():\n",
    "    # ... training code here ...\n",
    "    \n",
    "    # Log the model to the current run\n",
    "    mlflow.sklearn.log_model(\n",
    "        sk_model=model, \n",
    "        artifact_path=\"purchase-predictor-model\"\n",
    "    )\n",
    "```\n",
    "\n",
    "**3. Logged vs. Registered**\n",
    "- It is important to know the difference:\n",
    "- **Logging:** Happens during training. It’s like putting a prototype on a shelf in the back of the lab. It is tied to a specific \"Run ID.\"\n",
    "- **Registering:** Happens after you decide a model is good. It’s like moving that prototype to the showroom and giving it a name like \"Production_v1.\"\n",
    "\n",
    "**4. Why \"Logged Models\" are better than local files**\n",
    "- If you just save a model to your local computer (e.g., model.save()), it often breaks when you move it to a server. With MLflow Logging:\n",
    "- **Reproducibility:** It remembers exactly which version of Python you used.\n",
    "- **Ease of Use:** You can load the model back with one line of code: model = mlflow.sklearn.load_model(\"runs:/<run_id>/purchase-predictor-model\")\n",
    "- **Governance:** You can see who logged the model and when."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "32bb607a-5022-4cb9-874d-c2e9f3bf11f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### MLflow UI\n",
    "\n",
    "**1. The Experiments Sidebar**\n",
    "- On the right-hand side of your Databricks Notebook, you’ll see a beaker icon. Clicking this opens the \"Experiment Sidebar.\"\n",
    "- It shows a quick list of all recent runs.\n",
    "- You can see the Date, User, and Status (Success/Fail) at a glance.\n",
    "- It provides a direct link to the full MLflow UI.\n",
    "\n",
    "**2. The Main Experiments Page**\n",
    "- This is the \"Control Center\" for your Machine Learning project. Here, you can:\n",
    "- **Filter & Search:** Search for runs where metrics.accuracy > 0.85.\n",
    "- **Compare Runs:** Select multiple boxes and click \"Compare.\" This opens a side-by-side view that highlights exactly which parameters (like n_estimators) led to better results.\n",
    "- **Parallel Coordinates Plot:** A visual map showing how different \"knobs\" you turned (parameters) affected the final score.\n",
    "\n",
    "**3. The Run Details Page**\n",
    "- When you click on a specific \"Run Name,\" you go deeper. This page is divided into:\n",
    "- **Parameters:** Every setting you logged (e.g., learning_rate: 0.01).\n",
    "- **Metrics:** Your final scores. If you log metrics over time (like during training loops), the UI will automatically draw a Line Chart showing the model getting smarter.\n",
    "- **Artifacts:** This is the most important part. It shows the Logged Model folder, your requirements files, and any plots (like a Confusion Matrix) you saved.\n",
    "\n",
    "**4. The Model Registry UI**\n",
    "- Once you decide a model is ready for the real world, you use this tab to manage its \"Career.\"\n",
    "- **Versions:** It shows Version 1, Version 2, etc.\n",
    "- **Stages:** You can see a clear badge for \"Staging\" or \"Production.\" \n",
    "- **Transitions:** You can request to move a model from Staging to Production, and a manager can approve it right here in the UI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7f3af1c2-ca16-4828-b179-ea6c6d82f127",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Task 1: Train a Simple Regression Model and login to MLFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8772e62-c158-47a3-ac59-53b6e0dfdf82",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT count(*) FROM ecommerce_prod.gold.user_ml_features;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe2c512d-59b5-468b-bb40-67725330bf1d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Re-load the data into Pandas\n",
    "df = spark.table(\"ecommerce_prod.gold.user_ml_features\").toPandas()\n",
    "\n",
    "# Now check if it works\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc14219b-c7af-4590-8792-dbd7331f04f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# 1. Load your engineered features\n",
    "# We created this table in the Gold notebook\n",
    "df = spark.table(\"ecommerce_prod.gold.user_ml_features\").toPandas()\n",
    "\n",
    "# 2. Prepare Features (X) and Target (y)\n",
    "# We want to predict 'total_spend' based on behavior\n",
    "X = df[['interaction_count', 'weekend_ratio', 'avg_viewed_price', 'category_diversity']]\n",
    "y = df['total_spend']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 3. Start MLflow Tracking\n",
    "# This creates a \"Run\" in the Databricks Experiment UI\n",
    "with mlflow.start_run(run_name=\"User_Spend_Predictor\"):\n",
    "    \n",
    "    # Set Hyperparameters\n",
    "    n_estimators = 100\n",
    "    max_depth = 10\n",
    "    \n",
    "    # LOG PARAMETERS (The Settings)\n",
    "    mlflow.log_param(\"num_trees\", n_estimators)\n",
    "    mlflow.log_param(\"max_depth\", max_depth)\n",
    "    \n",
    "    # Train Model\n",
    "    rf = RandomForestRegressor(n_estimators=n_estimators, max_depth=max_depth)\n",
    "    rf.fit(X_train, y_train)\n",
    "    \n",
    "    # Make Predictions\n",
    "    predictions = rf.predict(X_test)\n",
    "    \n",
    "    # LOG METRICS (The Results)\n",
    "    mse = mean_squared_error(y_test, predictions)\n",
    "    r2 = r2_score(y_test, predictions)\n",
    "    \n",
    "    mlflow.log_metric(\"mse\", mse)\n",
    "    mlflow.log_metric(\"r2_score\", r2)\n",
    "    \n",
    "    # LOG MODEL (The Artifact)\n",
    "    # This packages the model so it can be used in SQL later\n",
    "    mlflow.sklearn.log_model(rf, \"spend_model\")\n",
    "    \n",
    "    print(f\"✅ Run Completed! R2 Score: {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d12b5da-5312-4ed3-b527-eaacd15c398f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# 1. Load your engineered features\n",
    "# We created this table in the Gold notebook\n",
    "df = spark.table(\"ecommerce_prod.gold.user_ml_features\").toPandas()\n",
    "\n",
    "# 2. Prepare Features (X) and Target (y)\n",
    "# We want to predict 'total_spend' based on behavior\n",
    "X = df[['interaction_count', 'weekend_ratio', 'avg_viewed_price', 'category_diversity']]\n",
    "y = df['total_spend']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 3. Start MLflow Tracking\n",
    "# This creates a \"Run\" in the Databricks Experiment UI\n",
    "with mlflow.start_run(run_name=\"User_Spend_Predictor\"):\n",
    "    \n",
    "    # Set Hyperparameters\n",
    "    n_estimators = 200\n",
    "    max_depth = 15\n",
    "    \n",
    "    # LOG PARAMETERS (The Settings)\n",
    "    mlflow.log_param(\"num_trees\", n_estimators)\n",
    "    mlflow.log_param(\"max_depth\", max_depth)\n",
    "    \n",
    "    # Train Model\n",
    "    rf = RandomForestRegressor(n_estimators=n_estimators, max_depth=max_depth)\n",
    "    rf.fit(X_train, y_train)\n",
    "    \n",
    "    # Make Predictions\n",
    "    predictions = rf.predict(X_test)\n",
    "    \n",
    "    # LOG METRICS (The Results)\n",
    "    mse = mean_squared_error(y_test, predictions)\n",
    "    r2 = r2_score(y_test, predictions)\n",
    "    \n",
    "    mlflow.log_metric(\"mse\", mse)\n",
    "    mlflow.log_metric(\"r2_score\", r2)\n",
    "    \n",
    "    # LOG MODEL (The Artifact)\n",
    "    # This packages the model so it can be used in SQL later\n",
    "    mlflow.sklearn.log_model(rf, \"spend_model\")\n",
    "    \n",
    "    print(f\"✅ Run Completed! R2 Score: {r2}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6419380467610544,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Day_12_MLflow_Basics",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
