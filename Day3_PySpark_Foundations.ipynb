{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c70dc426-7291-4b2a-910c-be4dfb940936",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### PySpark vs Pandas comparison\n",
    "\n",
    "- Pandas is like a personal laptop—great for individual tasks. PySpark is like a supercomputing lab—built to handle tasks that would crash a single machine.\n",
    "\n",
    "**1. High level comparision**\n",
    "\n",
    "| Feature |\tPandas |\tPySpark |\n",
    "|----|----|----|\n",
    "| Architecture |\tSingle Node: Runs on one machine. |\tDistributed: Runs on a cluster.\n",
    "| Memory\t| Loads all data into RAM. |\tDistributes data across Nodes.\n",
    "| Execution\t| Eager: Executes code line-by-line.\t| Lazy: Builds a plan (DAG) first.\n",
    "| Dataset Size\t| Small/Medium (up to ~10GB).\t| Big Data (TB to PB).\n",
    "| Performance |\tFaster for small data (no overhead).\t| Faster for massive data (parallelism).\n",
    "\n",
    "**2. Syntax Side-by-Side**\n",
    "\n",
    "| Operation | Pandas Syntax | PySpark Syntax\n",
    "| ----- | ----- | ----- |\n",
    "| Read CSV | pd.read_csv(\"file.csv\") | spark.read.csv(\"file.csv\", header=True)\n",
    "| Select Columns | df[['name', 'age']] | df.select(\"name\", \"age\")\n",
    "| Filter | df[df['age'] > 21] | df.filter(df.age > 21)\n",
    "| Add | Columndf['new'] = df['id']*2 | df.withColumn(\"new\", df.id*2)\n",
    "| GroupBy | df.groupby(\"cat\").sum() | df.groupBy(\"cat\").sum()\n",
    "| Renaming | df.rename(columns={'a':'b'}) | df.withColumnRenamed(\"a\", \"b\")\n",
    "| Missing Values | df.fillna(0) | df.fillna(0) or df.na.fill(0)\n",
    "\n",
    "**3. When should you switch?**\n",
    "\n",
    "In the world of Databricks, you’ll often use both, but the \"Rule of Thumb\" is:\n",
    "- Stay with Pandas if: Your data fits in your laptop's memory (usually < 2GB) and you need to do quick exploratory analysis or plotting.\n",
    "- Switch to PySpark if: Your dataset is larger than your available RAM (e.g., a 50GB file). The processing takes too long on a single machine. You are building a production pipeline that needs to scale as the business grows.\n",
    "\n",
    "**4. The \"Best of Both Worlds\" (Pandas API on Spark)**\n",
    "Since 2022, Databricks has integrated something called Pandas API on Spark (formerly known as Koalas). It allows you to write Pandas code but have it run on the Spark engine.\n",
    "\n",
    "```\n",
    "import pyspark.pandas as ps\n",
    "\n",
    "# This looks like Pandas, but runs distributed on a cluster!\n",
    "df = ps.read_csv(\"s3://massive-bucket/data.csv\")\n",
    "df_filtered = df[df['price'] > 100]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2551fc79-6ead-4369-963e-667875f6259c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Joins (inner, left, right, outer)\n",
    "\n",
    "**1. Join Types**\n",
    "| Join Type |\tWhat it returns\tVisual Diagram\n",
    "| ----- | -----\n",
    "| Inner |\tOnly the rows where there is a match in both tables.\t\n",
    "| Left (Outer) |\tAll rows from the left table, plus matching rows from the right. (Non-matches get null).\t\n",
    "| Right (Outer) |\tAll rows from the right table, plus matching rows from the left. (Non-matches get null).\t\n",
    "| Full Outer |\tAll rows from both tables. It fills in null wherever a match is missing\n",
    "\n",
    "**2. PySpark Syntax**\n",
    "\n",
    "In PySpark, the syntax is very consistent. The most important part is the how parameter.\n",
    "```\n",
    "# Basic Join Syntax\n",
    "joined_df = df_left.join(df_right, df_left.customer_id == df_right.user_id, how=\"inner\")\n",
    "\n",
    "# Common 'how' options:\n",
    "# \"inner\", \"left\", \"right\", \"outer\" (or \"full\")\n",
    "```\n",
    "\n",
    "**3. The Spark Secret: Broadcast Joins**\n",
    "\n",
    "- In a distributed environment, joining two massive tables requires a Shuffle, where Spark moves data between executors to find matches. This is slow.\n",
    "- However, if one of your tables is small (e.g., a 10MB \"Product Category\" table) and the other is huge (e.g., 1TB \"Sales\" table), Databricks can perform a Broadcast Join.\n",
    "- **How it works:** Spark sends the entire small table to every executor.\n",
    "- **The Benefit:** The large table doesn't have to move! The join happens locally on each machine.\n",
    "- **Result:** It can turn a 10-minute job into a 10-second job.\n",
    "- Databricks usually does this automatically (Auto-broadcast), but you can force it in your code:\n",
    "\n",
    "```\n",
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "# Tell Spark to send 'small_df' to all workers\n",
    "joined_df = large_df.join(broadcast(small_df), \"id\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fd3a355e-d0fd-4105-a81e-41f5b4952bfe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Window functions (running totals, rankings)\n",
    "\n",
    "- Windows functions allow you to perform calculations across a set of rows (a \"window\") that are related to the current row—without collapsing them into a single row like a groupBy does.\n",
    "- If a groupBy is like a summary report, a Window Function is like adding a \"Running Total\" or \"Rank\" column to your existing spreadsheet.\n",
    "\n",
    "**1. The Anatomy of a Window**\n",
    "- In PySpark, every window function requires a Window Specification. \n",
    "- This defines how the data is grouped and ordered before the calculation happens.\n",
    "```\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Define the \"Spec\"\n",
    "windowSpec = Window.partitionBy(\"category\").orderBy(\"sales_date\")\n",
    "```\n",
    "- partitionBy: Defines the groups (e.g., \"Calculate this for each product category\").\n",
    "- orderBy: Defines the sequence (e.g., \"Sort by date so we can calculate a trend\").\n",
    "- rowsBetween: (Optional) Defines the \"frame\" or boundaries (e.g., \"Only look at the last 7 days\").\n",
    "\n",
    "**2. Ranking Functions**\n",
    "- Ranking is essential for \"Top N\" problems (e.g., \"Find the top 3 best-selling products in every region\").\n",
    "| Function | Behavior | Result for tied values (10, 10, 12)\n",
    "| ----- | ----- | ----- |\n",
    "| row_number() | Unique sequential number.| 1, 2, 3\n",
    "| rank() | Leaves gaps after ties. | 1, 1, 3\n",
    "| dense_rank() | No gaps after ties. | 1, 1, 2\n",
    "\n",
    "Example: Ranking products by price within each category\n",
    "```\n",
    "Pythondf.withColumn(\"rank\", F.dense_rank().over(Window.partitionBy(\"category\").orderBy(F.desc(\"price\"))))\n",
    "```\n",
    "\n",
    "**3. Running Totals (Cumulative Sum)**\n",
    "- To calculate a running total, you use a standard aggregation function (like sum) but apply it over a window.\n",
    "- By default, if you provide an orderBy in your window spec, Spark assumes you want a running total from the start of the partition up to the current row.\n",
    "- Example: Cumulative sales per day\n",
    "```\n",
    "windowSpec = Window.partitionBy(\"store_id\").orderBy(\"date\")\n",
    "\n",
    "df.withColumn(\"running_total\", F.sum(\"daily_sales\").over(windowSpec))\n",
    "```\n",
    "\n",
    "**4. Analytical Functions (Lead & Lag)**\n",
    "- These functions allow you to \"look ahead\" or \"look back\" at previous or future rows. This is incredibly useful for calculating **month-over-month growth.**\n",
    "- **lag(col, 1):** Pulls the value from the previous row.\n",
    "- **lead(col, 1):** Pulls the value from the next row.\n",
    "- Example: Calculating Daily Growth\n",
    "```\n",
    "df.withColumn(\"prev_day_sales\", F.lag(\"sales\").over(windowSpec)).withColumn(\"growth\", F.col(\"sales\") - F.col(\"prev_day_sales\"))\n",
    "```\n",
    "\n",
    "**5. Summary Table**\n",
    "| Use Case | Recommended Function |\n",
    "| ---- | ---- |\n",
    "| Top 3 items per category | dense_rank() \n",
    "| Cumulative Revenue | sum().over(window)\n",
    "| Year-over-Year (YoY) | Growthlag()\n",
    "| Moving Average (7-day) | avg().over(window.rowsBetween(-6, 0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "15ae43ed-d07a-4e8a-9e24-1f51568ebd4b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### User-Defined Functions (UDFs)\n",
    "\n",
    "Think of a UDF as a custom-made tool you build when the standard Spark toolbox doesn't have exactly what you need.\n",
    "\n",
    "**1. How a UDF Works**\n",
    "- When you define a UDF in Python, you are essentially telling Spark: \"Take this Python function and apply it to every row in this column.\"\n",
    "\n",
    "**The Python Syntax**\n",
    "```\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# 1. Define a standard Python function\n",
    "def categorize_price(price):\n",
    "    if price > 100: return \"Premium\"\n",
    "    else: return \"Budget\"\n",
    "\n",
    "# 2. Register it as a UDF\n",
    "price_udf = udf(categorize_price, StringType())\n",
    "\n",
    "# 3. Use it in your DataFrame\n",
    "df.withColumn(\"category\", price_udf(df.price)).show()\n",
    "```\n",
    "\n",
    "**2. The \"Python Tax\" (Performance Warning)**\n",
    "- Standard Python UDFs are notoriously slow. To understand why, we have to look at how Spark (which runs on the JVM) talks to Python.\n",
    "- Serialization: Spark must \"pickle\" (serialize) the data into bytes.\n",
    "- Data Movement: It sends those bytes from the JVM to a Python process on the worker node.\n",
    "- Execution: Python runs the function row-by-row.\n",
    "- Return: The result is serialized again and sent back to the JVM.\n",
    "- The Golden Rule of Spark: Always check if a built-in function (like when().otherwise()) can do the job before reaching for a UDF. Built-in functions run directly in the JVM and are significantly faster.\n",
    "\n",
    "**3. The Solution: Pandas UDFs (Vectorized)**\n",
    "- To solve the performance bottleneck, Spark introduced Pandas UDFs. These use Apache Arrow to move data between the JVM and Python much more efficiently.\n",
    "- Standard UDF: Processes data row-by-row.\n",
    "- Pandas UDF: Processes data in batches (vectors).\n",
    "- Pandas UDF Syntax\n",
    "```\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "\n",
    "@pandas_udf(\"string\")\n",
    "def vectorized_categorize(price_series: pd.Series) -> pd.Series:\n",
    "    # This runs on a whole batch of prices at once!\n",
    "    return price_series.apply(lambda x: \"Premium\" if x > 100 else \"Budget\")\n",
    "\n",
    "df.withColumn(\"category\", vectorized_categorize(df.price))\n",
    "```\n",
    "\n",
    "**4. Comparison Table**\n",
    "\n",
    "| Feature | Standard Python UDF | Pandas UDF (Vectorized) | Built-in Functions \n",
    "| ----- | ----- | ----- | ----- \n",
    "| Performance | Slow (Row-by-row) | Fast (Batched) | Fastest (Native JVM)\n",
    "| Complexity | Easy to write. | Requires Pandas knowledge. | Easiest (if available).\n",
    "| Use Case | Complex, non-math logic. | Statistical/ML operations. | 90% of standard ETL.\n",
    "\n",
    "**5. When to use UDFs in Databricks**\n",
    "- Use Built-in Functions: For 95% of tasks (math, string cleaning, date manipulation).\n",
    "- Use Pandas UDFs: When you need to use libraries like scipy, numpy, or statsmodels on your data.\n",
    "- Use Standard UDFs: Only when the logic is highly specialized, doesn't involve heavy math, and cannot be vectorized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86559281-663c-4630-86a0-46bbdeb7de7e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## load full ecommerce dataset\n",
    "\n",
    "# Define the path to your downloaded CSV\n",
    "file_path = \"/Volumes/workspace/ecommerce/ecommerce_data/2019-Oct.csv\"\n",
    "\n",
    "# Read the file with correct options\n",
    "df = (spark.read\n",
    "      .format(\"csv\")\n",
    "      .option(\"header\", \"true\")        # Uses the first row as column names\n",
    "      .option(\"inferSchema\", \"true\")   # Automatically detects data types (e.g., price as double)\n",
    "      .load(file_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6d9915d-d4a5-49f5-a41e-fc37c7ba8728",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define the path to your downloaded CSV\n",
    "file_path = \"/Volumes/workspace/ecommerce/ecommerce_data/2019-Nov.csv\"\n",
    "\n",
    "# Read the file with correct options\n",
    "df_n = (spark.read\n",
    "      .format(\"csv\")\n",
    "      .option(\"header\", \"true\")        # Uses the first row as column names\n",
    "      .option(\"inferSchema\", \"true\")   # Automatically detects data types (e.g., price as double)\n",
    "      .load(file_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "891c8898-5bbe-4079-944e-cd18f9029184",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_n.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a1533ea-34e3-4bd2-9d8d-6815f14fa524",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_oct = df.limit(1000)\n",
    "df_nov = df_n.limit(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb9fcf7a-a89a-4445-a810-4f7e5eab84f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## joins\n",
    "\n",
    "## inner join\n",
    "## Returns only products sold in both months\n",
    "inner_df = df_oct.join(df_nov, on=\"product_id\", how=\"inner\")\n",
    "inner_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53ee29e3-b408-4e33-be6c-a34a466a2db1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## left join\n",
    "## keeps all the products from October\n",
    "left_df = df_oct.join(df_nov, on=\"product_id\", how=\"left\")\n",
    "left_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc7e1498-43e1-4f94-9b5d-7d9a032852f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## full join\n",
    "## Every product that appeared in either month\n",
    "\n",
    "full_df = df_oct.join(df_nov, on=\"product_id\", how=\"full\")\n",
    "full_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "441b741a-a0ec-4684-a9d9-17dd6c9f443e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## semi join\n",
    "## October sales for products that we also managed to sell in November.\n",
    "\n",
    "semi_df = df_oct.join(df_nov, on=\"product_id\", how=\"semi\")\n",
    "semi_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78eb1ef3-7870-458e-adfe-281ff449e527",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## products we sold in October but lost in November\n",
    "anti_df = df_oct.join(df_nov, on=\"product_id\", how=\"anti\")\n",
    "anti_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a2135ad-5565-42f0-abeb-cb7f8b64a02a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## aliasing\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "joined_df = df_oct.alias(\"oct\").join(\n",
    "    df_nov.alias(\"nov\"),\n",
    "    on=\"product_id\",\n",
    "    how=\"inner\"\n",
    ").select(\n",
    "    \"product_id\",\n",
    "    F.col(\"oct.price\").alias(\"oct_price\"),\n",
    "    F.col(\"nov.price\").alias(\"nov_price\")\n",
    ")\n",
    "\n",
    "joined_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39a33b36-f8cc-4dd1-843f-134491aff53e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## cross join\n",
    "\n",
    "cross_df = df_oct.crossJoin(df_nov)\n",
    "cross_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2850cf7-f804-4808-a1d1-ea2c2f93b996",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## cumulative revenue per day\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "## group by date to get daily totals\n",
    "daily_sales = df.groupBy(\"event_time\").agg(F.sum(\"price\").alias(\"daily_revenue\"))\n",
    "\n",
    "## define window specification\n",
    "window_spec = Window.orderBy(\"event_time\")\n",
    "\n",
    "## apply the running total\n",
    "running_total_df = daily_sales.withColumn(\"running_total\", F.sum(\"daily_revenue\").over(window_spec))\n",
    "running_total_df.orderBy(\"event_time\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a072faf9-f2f7-4da5-a4ed-2d078762e85b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## time based features\n",
    "\n",
    "df_features = df.withColumn(\"event_time\", F.to_timestamp(\"event_time\")).withColumn(\n",
    "    \"hour\", F.hour(\"event_time\")).withColumn(\n",
    "        \"day_of_week\", F.dayofweek(\"event_time\")).withColumn(\n",
    "            \"is_weekend\", F.when(F.col(\"day_of_week\").isin(1,7),1).otherwise(0))\n",
    "\n",
    "df_features.limit(10).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f224b46-2d9f-4931-9421-5efc9cc8b5b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# define cast window by category\n",
    "cast_window = Window.partitionBy(\"category_id\")\n",
    "\n",
    "df_features = df_features.withColumn(\"avg_category_price\", F.avg(\"price\").over(cast_window)).withColumn(\n",
    "    \"price_diff_from_avg\", F.col(\"price\") - F.col(\"avg_category_price\")\n",
    ")\n",
    "\n",
    "df_features.select(\"avg_category_price\", \"price_diff_from_avg\").limit(10).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0fc86a3e-ad71-4a89-b5f5-b7ccb17e8e2e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Day3_PySpark_Foundations",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
