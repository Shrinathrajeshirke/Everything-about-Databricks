{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "daf80967-6a61-4cfd-b843-083dc7be2087",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Time Travel (Version history)\n",
    "\n",
    "- Time Travel is one of the most popular features of Delta Lake. It allows you to query a table as it existed at a specific point in time or at a specific \"version.\"\n",
    "- Every time you change a Delta table (like your MERGE or INSERT tasks), Delta creates a new version. \n",
    "- It doesn’t delete the old data; it just keeps track of the changes in the Transaction Log.\n",
    "\n",
    "**1. How to see your History**\n",
    "- First, you look at the \"Log Book\" to see every change ever made to the table.\n",
    "```\n",
    "SQL\n",
    "\n",
    "DESCRIBE HISTORY student_grades;\n",
    "```\n",
    "```\n",
    "Python\n",
    "\n",
    "from delta.tables import DeltaTable\n",
    "history = DeltaTable.forName(spark, \"student_grades\").history()\n",
    "display(history)\n",
    "```\n",
    "This shows you a list: Version 0 (Original), Version 1 (Updated), Version 2 (Deleted), etc.\n",
    "\n",
    "**2. How to \"Time Travel\"**\n",
    "- Imagine a student named \"Sam\" had a grade of 85. You updated it to 95, but then realized the first grade was actually correct. You can query the table exactly as it was before the change.\n",
    "\n",
    "- By Version Number:\n",
    "```\n",
    "SQL\n",
    "\n",
    "-- See the table exactly as it was at the very start\n",
    "SELECT * FROM student_grades VERSION AS OF 0;\n",
    "```\n",
    "```\n",
    "By Timestamp:\n",
    "\n",
    "SQL\n",
    "\n",
    "-- See the grades as they were at 10:00 AM this morning\n",
    "SELECT * FROM student_grades TIMESTAMP AS OF '2026-01-13 10:00:00';\n",
    "```\n",
    "\n",
    "**3. Why is this useful?**\n",
    "- Undo Mistakes: If you accidentally delete all students from the table, you don't lose the data. You just \"travel back\" to the version before the deletion.\n",
    "- Comparison: You can compare today's grades with last month's grades side-by-side using two different versions of the same table.\n",
    "- Audit: If someone asks, \"Why did Sam have an 85 last week?\", you can pull up the exact data from last week to prove it.\n",
    "\n",
    "**4. How to Restore (The \"Undo\" Button)**\n",
    "- If you want to permanently bring back an old version because the current one is wrong:\n",
    "```\n",
    "SQL\n",
    "\n",
    "-- This permanently moves the table back to Version 0\n",
    "RESTORE TABLE student_grades TO VERSION AS OF 0;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a7a8cf9f-1fb1-4f27-814f-6e35c79924f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### MERGE Operations (upsert)\n",
    "- A MERGE operation is like a \"Smart Update.\" Instead of just adding new data (which creates duplicates) or overwriting everything (which is slow), Delta Lake looks at your table and decides row-by-row what to do.\n",
    "- Think of it as a \"Search and Act\" command.\n",
    "\n",
    "**1. The Logic: \"Find, then Decide\"**\n",
    "- Imagine you have a Staff List table and you receive a new list of updates today.\n",
    "- Search: Delta compares the New List to the Existing Table using a unique ID (like employee_id).\n",
    "- Match Found: If the ID exists, it Updates the person's info (e.g., they got a promotion).\n",
    "- No Match: If the ID is new, it Inserts them as a new employee.\n",
    "\n",
    "**2. How to do it in SQL**\n",
    "- This is the most common way to handle \"Upserts\" (Update + Insert).\n",
    "```\n",
    "SQL\n",
    "\n",
    "%sql\n",
    "MERGE INTO staff_table AS target\n",
    "USING updates_df AS source\n",
    "ON target.employee_id = source.employee_id\n",
    "WHEN MATCHED THEN\n",
    "  UPDATE SET \n",
    "    target.salary = source.salary,\n",
    "    target.role = source.role\n",
    "WHEN NOT MATCHED THEN\n",
    "  INSERT (employee_id, name, salary, role)\n",
    "  VALUES (source.employee_id, source.name, source.salary, source.role);\n",
    "```\n",
    "\n",
    "**3. How to do it in PySpark**\n",
    "- PySpark gives you more control if you want to write the logic programmatically.\n",
    "```\n",
    "Python\n",
    "\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "# 1. Load the existing table\n",
    "staff_table = DeltaTable.forName(spark, \"staff_table\")\n",
    "\n",
    "# 2. Run the merge\n",
    "staff_table.alias(\"target\") \\\n",
    "  .merge(\n",
    "    source = updates_df.alias(\"source\"),\n",
    "    condition = \"target.employee_id = source.employee_id\"\n",
    "  ) \\\n",
    "  .whenMatchedUpdateAll() \\\n",
    "  .whenNotMatchedInsertAll() \\\n",
    "  .execute()\n",
    "```\n",
    "\n",
    "**4. Why is this a \"Superpower\"?**\n",
    "- No Duplicates: It automatically prevents you from having two rows for the same person.\n",
    "- Efficiency: Delta Lake doesn't rewrite the whole table. It only changes the specific files that contain the data you are updating.\n",
    "- Reliability: If the computer crashes halfway through the Merge, Delta Lake uses its \"Transaction Log\" to ensure the table stays perfect—it’s all or nothing.\n",
    "\n",
    "**5. Common Mistake to Avoid**\n",
    "- The \"Unresolved\" Error: As you saw earlier, if you try to Merge using a column that doesn't exist (like order_id when the column is actually product_id), the operation will fail. Always check your column names first!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c77ffee0-0fe9-4b4e-85b5-f8080e37400f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### OPTIMIZE & ZORDER\n",
    "- As your data grows, even a fast Delta table can slow down. \n",
    "- OPTIMIZE and ZORDER are the two \"clean-up\" tools you use to keep your queries running at lightning speed.\n",
    "- Think of it like organizing a messy library so you can find a specific book in seconds instead of hours.\n",
    "\n",
    "**1. OPTIMIZE (Compaction)**\n",
    "- When you write data frequently, Spark creates many tiny files. Reading 1,000 tiny files is much slower than reading one big file.\n",
    "- OPTIMIZE takes all those small \"splinter\" files and compacts them into larger, more efficient chunks.\n",
    "```\n",
    "SQL\n",
    "\n",
    "OPTIMIZE student_grades;\n",
    "```\n",
    "\n",
    "**2. Z-ORDER (Data Clustering)**\n",
    "- Even with big files, Spark might still have to scan the whole file to find one student. \n",
    "- Z-ORDER physically rearranges the data inside the files so that similar information is stored together.\n",
    "- If you often search for students by their subject_id, Z-Ordering by that column tells Spark exactly which part of the file to skip.\n",
    "```\n",
    "SQL\n",
    "\n",
    "OPTIMIZE student_grades\n",
    "ZORDER BY (subject_id);\n",
    "```\n",
    "\n",
    "**3. How they work together (The Library Analogy)**\n",
    "- Without Optimize: Your books are scattered in 100 tiny boxes. You have to open every box to find anything.\n",
    "- With Optimize: You dump all the books into 5 large, sturdy shelves.\n",
    "- With Z-Order: You arrange the books on those shelves alphabetically. Now, if you need a book starting with \"Z,\" you skip the first 4 shelves entirely.\n",
    "\n",
    "**4. When should you use them?**\n",
    "- OPTIMIZE: Run this once a day or once a week to clean up \"fragmented\" data.\n",
    "- Z-ORDER: Use this only on columns you use most often in your WHERE clauses (like user_id, date, or product_id)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "97e2d1f0-7344-4c83-ab7a-87e0e32a51cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### VACUUM for cleanup\n",
    "- While Time Travel is a superpower that lets you see old versions of your data, those old files take up storage space and cost money. \n",
    "- VACUUM is the \"cleanup crew\" that permanently deletes those old files once you no longer need them.\n",
    "- Think of it like emptying the Recycle Bin on your computer.\n",
    "\n",
    "**1. What does VACUUM do?**\n",
    "- When you update or delete data in Delta Lake, the old files aren't immediately erased (so you can Time Travel). \n",
    "- VACUUM looks for files that are no longer part of the \"latest\" version of the table and have been \"expired\" for a certain amount of time.\n",
    "\n",
    "**2. The \"Retention Period\"**\n",
    "- By default, Delta Lake won't let you vacuum files that are less than 7 days old. \n",
    "- This is a safety feature to make sure you don't delete data that someone might still be querying or trying to time travel to.\n",
    "- SQL Example:\n",
    "```\n",
    "SQL\n",
    "\n",
    "-- Remove files older than the default 7 days\n",
    "VACUUM student_grades;\n",
    "\n",
    "-- Remove files older than 100 hours (roughly 4 days)\n",
    "VACUUM student_grades RETAIN 100 HOURS;\n",
    "```\n",
    "\n",
    "**3. The Library Analogy (Updated)**\n",
    "- Time Travel: You keep every old edition of a textbook in the basement just in case.\n",
    "- VACUUM: You realize the basement is full and costs too much to rent. You decide to throw away any edition older than 2020.\n",
    "- Warning: Once you throw them away, you can no longer \"Time Travel\" back to see them!\n",
    "\n",
    "**4. Important Warning**\n",
    "- Once you run VACUUM, the old data is permanently gone.\n",
    "- If you try to run a Time Travel query on a version that has been vacuumed, you will get an error saying the underlying files are missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4bae727d-ed8c-41eb-acad-c19388c1f225",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## creating a employee table\n",
    "\n",
    "data = [\n",
    "  (1, \"Alice\", \"HR\", 5000),\n",
    "  (2, \"Bob\", \"IT\", 6000),\n",
    "  (3, \"Charlie\", \"Sales\", 4500)\n",
    "]\n",
    "\n",
    "columns = [\"emp_id\", \"name\", \"dept\", \"salary\"]\n",
    "\n",
    "## create dataframe and write as a delta\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"employee\")\n",
    "\n",
    "print(\"initial data table created as version 0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33d68440-fb3c-4b6f-8f96-55fa1c4202ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Task 1: Implement Incremental MERGE\n",
    "\n",
    "new_data = [\n",
    "    (1, \"Alice\", \"HR\", 5500), # Update: Salary increased\n",
    "    (4, \"David\", \"IT\", 5000)   # New: new employee\n",
    "]\n",
    "\n",
    "updates_df = spark.createDataFrame(new_data, columns)\n",
    "updates_df.createOrReplaceTempView(\"updates_view\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82ad6aba-27a8-4ddf-9ed2-bfcaef452764",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "MERGE into employee as target\n",
    "using updates_view as source\n",
    "on target.emp_id = source.emp_id\n",
    "when matched then \n",
    "update set target.salary = source.salary\n",
    "when not matched then \n",
    "insert *;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d2f07497-9858-4215-93b5-27369d0a5131",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Task 2 - time travel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dca899f8-6499-467e-9359-bb9ebf6de4c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- see the history\n",
    "describe history employee;\n",
    "\n",
    "-- query version 0\n",
    "select * from employee version as of 0;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fe8f9856-ddb2-45d3-9efb-9ef2760741f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Task 3 - optimize tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9627246f-7319-4372-bc6b-09f60054f65a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "OPTIMIZE employee  \n",
    "ZORDER BY (dept);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a6539719-445d-4a30-924e-6aa6be26a472",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Task 4 - clean old files: VACUUM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cae99781-8b6d-4974-9bfc-844e0502cc15",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- see which files are no longer required\n",
    "VACUUM employee DRY RUN;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9b6c3b4-9471-4382-b242-98de608dc1e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- To actually delete (requires changing a setting for small retention)\n",
    "-- SET spark.databricks.delta.retentionDurationCheck.enabled = false;\n",
    "-- VACUUM employees RETAIN 0 HOURS;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6b62d47f-0ff8-425f-bde0-23bc0ca62fba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6614041501988482,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Day_5_Delta_Lake_Adv",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
