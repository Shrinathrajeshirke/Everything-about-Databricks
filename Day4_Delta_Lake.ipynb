{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "05cc7182-59b8-4efa-89df-219d22f00d99",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Delta lake\n",
    "\n",
    "Delta Lake is an open-source storage layer that brings reliability to Big Data lakes. It sits on top of your existing cloud storage (like S3 or ADLS) and provides the structure and speed of a Data Warehouse with the flexibility of a Data Lake.\n",
    "\n",
    "**1. Why do we need it?**\n",
    "- In a standard Data Lake, you are just saving raw files (CSV, Parquet, JSON). This causes three major problems:\n",
    "- Failed Jobs: If a job crashes halfway through, you end up with \"garbage\" partial data.\n",
    "- No Updates: You can't easily run an UPDATE or DELETE on a raw Parquet file.\n",
    "- Data Quality: There is no way to stop \"bad data\" from being written into your lake.\n",
    "\n",
    "**2. The Core Features (ACID)**\n",
    "- Delta Lake solves these problems using four main pillars:\n",
    "- **ACID Transactions:** (Atomicity, Consistency, Isolation, Durability). This ensures that a job either succeeds completely or fails completely. No more partial data.\n",
    "- **Scalable Metadata:** It uses a Transaction Log (the _delta_log folder) to keep track of every file. This makes listing files in a massive directory near-instant.\n",
    "- **Time Travel:** Because Delta keeps a history of changes, you can query your data as it existed at a specific point in time.\n",
    "- Example: SELECT * FROM table VERSION AS OF 5\n",
    "- **Schema Enforcement:** It prevents \"dirty data\" from entering your table. If you try to write a String into an Integer column, Delta will block it.\n",
    "\n",
    "**3. The Medallion Architecture**\n",
    "- Databricks uses Delta Lake to organize data into three \"Zones\":\n",
    "- **Bronze (Raw):** The landing zone. Data is kept in its rawest form.\n",
    "- **Silver (Cleaned):** Data is filtered, joined, and standardized. Great for Data Scientists.\n",
    "- **Gold (Aggregated):** Final, business-ready tables (e.g., \"Monthly Revenue\"). Ready for Power BI or Tableau.\n",
    "\n",
    "**4. Simple Syntax**\n",
    "- The best part is that Delta uses standard SQL/PySpark syntax. You just change the format from parquet to delta.\n",
    "```\n",
    "# Writing data as Delta\n",
    "df.write.format(\"delta\").save(\"/mnt/delta/orders\")\n",
    "\n",
    "# Updating data (Something you can't do in standard Spark!)\n",
    "from delta.tables import DeltaTable\n",
    "deltaTable = DeltaTable.forPath(spark, \"/mnt/delta/orders\")\n",
    "deltaTable.update(\"id = 123\", {\"status\": \"'shipped'\"})\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a02ccf9d-890a-40e5-989c-dbd136275e55",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### ACID\n",
    "\n",
    "#### The Four Pillars of ACID\n",
    "\n",
    "**1. Atomicity (All or Nothing)**\n",
    "- Atomicity ensures that a transaction is \"atomic\"—it either succeeds completely or fails completely.\n",
    "- The Benefit: If your Databricks cluster terminates in the middle of a 10GB write, Delta Lake will automatically roll back the changes. To the user, it will look as if the write never started, leaving your table clean.\n",
    "\n",
    "**2. Consistency (Rules are Followed)**\n",
    "- Consistency ensures that the data moves from one valid state to another, following all defined rules (like Schema Enforcement).\n",
    "- The Benefit: If you try to write a \"String\" into a column defined as an \"Integer,\" Delta Lake will block the transaction. This prevents \"data corruption\" where different rows have different data types.\n",
    "\n",
    "**3. Isolation (No Interference)**\n",
    "- Isolation ensures that multiple users can read and write to the same table at the exact same time without seeing each other's partial work.\n",
    "- The Benefit: A Data Analyst can run a report on your Sales table while a Data Engineer is currently appending new November data. The Analyst will see a \"consistent snapshot\" of the data as it was before the update started.\n",
    "\n",
    "**4. Durability (It Stays Saved)**\n",
    "- Durability guarantees that once a transaction is committed, it remains committed, even in the event of a system failure or power loss.\n",
    "- The Benefit: In Delta Lake, a transaction is only \"committed\" once it is written to the Transaction Log (_delta_log). Once it's in that log, it is permanent.\n",
    "\n",
    "**How it works: The Transaction Log**\n",
    "- The secret to ACID in Delta Lake is the Transaction Log (also called the DeltaLog).\n",
    "- Recording Intent: When you start a write, Delta creates a new JSON file in the _delta_log folder.\n",
    "- Tracking Files: It records exactly which new Parquet files were added and which old ones were removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f3e04214-c641-4a8c-95a9-a45b146db4ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Schema enforcement\n",
    "\n",
    "- It ensures data quality by preventing \"poisonous\" data from breaking your tables.\n",
    "- In a traditional Data Lake, if you try to write a file with an extra column or a different data type, the write succeeds, but your downstream dashboards or ML models crash later. Schema Enforcement stops this at the source.\n",
    "\n",
    "**1. How it Works**\n",
    "- When a write operation is triggered, Delta Lake compares the schema of the incoming DataFrame to the existing table's schema.\n",
    "- **Data Types:** If the table expects an Integer but you send a String, the write fails.\n",
    "- **Column Names:** If you send a column that doesn't exist in the table, the write fails.\n",
    "- **Missing Columns:** If you send fewer columns than the table has, Delta allows it but fills the missing values with null (unless specified otherwise).\n",
    "\n",
    "**2. The Benefit: No More \"Data Swamps\"**\n",
    "- Without enforcement, data lakes tend to get messy over time as different teams upload files with slightly different formats.\n",
    "| Feature | Standard Parquet/CSV | Delta Lake |\n",
    "|----- | ----- | ----- |\n",
    "|New Column | Added silently (can break code). | Blocked (unless explicitly allowed).\n",
    "| Wrong Data Type | Written as-is (causes crash later). | Immediate Error at write-time.\n",
    "| Case Sensitivity | Varies by engine. | Strictly enforced by the Transaction Log.\n",
    "\n",
    "**3. What if you want to change the schema? (Schema Evolution)**\n",
    "- Sometimes, your data naturally changes (e.g., you start collecting a new discount_code field). You can explicitly tell Delta to allow this change using Schema Evolution.\n",
    "- By adding .option(\"mergeSchema\", \"true\"), you tell Spark: \"I know the schema is different; please update the table to match my new data.\"\n",
    "```\n",
    "# This will fail if the schema doesn't match\n",
    "df.write.format(\"delta\").mode(\"append\").save(path)\n",
    "\n",
    "# This will succeed and update the table with new columns\n",
    "df.write.format(\"delta\").mode(\"append\") \\\n",
    "  .option(\"mergeSchema\", \"true\") \\\n",
    "  .save(path)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ad6ba24b-2d84-411d-ad6c-ee150b0dabb3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Delta vs Parquet\n",
    "\n",
    "- Think of Parquet as the bricks and Delta Lake as the entire building.\n",
    "- Delta Lake uses Parquet files to store the actual data, but adds a \"Brain\" (the Transaction Log) on top to manage them.\n",
    "\n",
    "**1. Key Differences at a Glance**\n",
    "\n",
    "|Feature | Vanilla Parquet | Delta Lake\n",
    "| ----- | ----- | ----- |\n",
    "| Data Storage | Columnar Files (.parquet) | Columnar Files + Transaction Log (_delta_log)\n",
    "| Transactions | None (crashes cause \"dirty\" data) | ACID (it works or it fails; no mess)\n",
    "| Updates/Deletes | Must rewrite the whole table | Easy UPDATE, DELETE, and MERGE\n",
    "| History | Latest version only | Time Travel (query past versions)\n",
    "| Data Quality | None (allows wrong data types) | Schema Enforcement (blocks bad data)\n",
    "| Performance | Basic (limited file skipping) | Advanced (Z-Ordering & fast metadata)\n",
    "\n",
    "**2. The \"Hidden\" Problem with Parquet**\n",
    "- If you have a folder full of Parquet files, Spark has to \"list\" all those files every time you run a query. \n",
    "- If you have 10,000 files in the cloud (S3/Azure), this \"listing\" can take minutes before the query even starts.\n",
    "- Delta Lake solves this by storing the list of files in its Transaction Log. Spark just reads that one small log file and knows exactly where to go. This makes it significantly faster as your data grows.\n",
    "\n",
    "**3. Visual Comparison**\n",
    "- Parquet: Just a bunch of files. If a write fails, you don't know which files are good or bad.\n",
    "- Delta: A \"Managed\" collection. The log acts as a checklist. If a file isn't on the checklist, Spark ignores it.\n",
    "\n",
    "**4. When to use which?**\n",
    "- Use Parquet for static, one-time exports or sharing data with legacy systems that don't support Delta.\n",
    "- Use Delta for almost everything else in Databricks—especially if your data changes, needs to be updated, or requires high reliability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e792963-8966-4fda-987b-d80aec01ec8f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## create a csv to delta format\n",
    "\n",
    "## define file path\n",
    "file_path =  \"/Volumes/workspace/ecommerce/ecommerce_data/2019-Oct.csv\"\n",
    "\n",
    "## read csv\n",
    "df = spark.read.format(\"csv\").option(\"header\",\"true\").option(\"inferSchema\",\"true\").load(file_path)\n",
    "\n",
    "## write to delta\n",
    "df.write.format(\"delta\").save(\"/Volumes/workspace/ecommerce/ecommerce_data/delta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe47d724-2b4c-4418-b354-d487dd008e28",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## create delta table (SQL and PySpark)\n",
    "\n",
    "## Using PySpark\n",
    "df.write.format(\"delta\").saveAsTable(\"sales_october\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e28f0d0-afc8-4064-9d20-770b6197082f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Using SQL"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE TABLE sales_october\n",
    "USING DELTA\n",
    "LOCATION '/Volumes/workspace/ecommerce/ecommerce_data/delta';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4901377d-1407-403f-9b22-fc1f6dd5ff6d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## taste schema enforcement\n",
    "\n",
    "# Create a dummy row with a NEW column 'extra_info'\n",
    "new_data = spark.createDataFrame([(\"user_1\", \"2026-01-12\", \"oops\")], [\"user_id\", \"event_time\", \"extra_info\"])\n",
    "\n",
    "# Try to append to your existing table\n",
    "# This will FAIL and throw a 'schema mismatch' error\n",
    "new_data.write.format(\"delta\").mode(\"append\").saveAsTable(\"sales_october\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9abd03e3-82fb-4cbb-8150-4228d5eb9186",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"updates_df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4955c263-f4db-4e2f-87d6-f4c60d1dc1cc",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "MERGE with row-level deduplication"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Ensure only one source row per key for MERGE\n",
    "CREATE OR REPLACE TEMP VIEW updates_df_dedup AS\n",
    "SELECT * FROM (\n",
    "  SELECT *, ROW_NUMBER() OVER (\n",
    "    PARTITION BY event_time, user_id, product_id ORDER BY event_time\n",
    "  ) AS rn\n",
    "  FROM updates_df\n",
    ") WHERE rn = 1;\n",
    "\n",
    "MERGE INTO sales_october AS target\n",
    "USING updates_df_dedup AS source\n",
    "ON target.event_time = source.event_time \n",
    "   AND target.user_id = source.user_id \n",
    "   AND target.product_id = source.product_id\n",
    "WHEN MATCHED THEN\n",
    "  UPDATE SET *\n",
    "WHEN NOT MATCHED THEN\n",
    "  INSERT *;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "af7959da-2d5c-4162-8b0f-233d7eefb72d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8545090578669375,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Day4_Delta_Lake",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
